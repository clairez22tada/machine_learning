{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Name: {Claire Zhou}\n",
    "## Section: {02}\n",
    "\n",
    "# Lab 8: Exploring Random Forests\n",
    "\n",
    "## Tools\n",
    "\n",
    "#### Libraries:\n",
    "\n",
    "- numpy: for processing\n",
    "- sklearn: for model training  \n",
    "- pandas: for data processing  \n",
    "- rfpimp **version 1.3.7**: for feature importance\n",
    "\n",
    "#### Datasets:\n",
    "\n",
    "Boston housing \n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Retrieving notices: ...working... done\n",
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: done\n",
      "\n",
      "## Package Plan ##\n",
      "\n",
      "  environment location: /opt/homebrew/anaconda3\n",
      "\n",
      "  added / updated specs:\n",
      "    - rfpimp\n",
      "\n",
      "\n",
      "The following packages will be downloaded:\n",
      "\n",
      "    package                    |            build\n",
      "    ---------------------------|-----------------\n",
      "    conda-23.10.0              |  py311hca03da5_0         1.3 MB\n",
      "    rfpimp-1.3.2               |             py_0          12 KB  conda-forge\n",
      "    ------------------------------------------------------------\n",
      "                                           Total:         1.3 MB\n",
      "\n",
      "The following NEW packages will be INSTALLED:\n",
      "\n",
      "  rfpimp             conda-forge/noarch::rfpimp-1.3.2-py_0 \n",
      "\n",
      "The following packages will be SUPERSEDED by a higher-priority channel:\n",
      "\n",
      "  certifi            conda-forge/noarch::certifi-2023.7.22~ --> pkgs/main/osx-arm64::certifi-2023.7.22-py311hca03da5_0 \n",
      "  conda              conda-forge::conda-23.10.0-py311h267d~ --> pkgs/main::conda-23.10.0-py311hca03da5_0 \n",
      "  ruamel.yaml        conda-forge::ruamel.yaml-0.18.5-py311~ --> pkgs/main::ruamel.yaml-0.17.21-py311h80987f9_0 \n",
      "\n",
      "\n",
      "\n",
      "Downloading and Extracting Packages:\n",
      "conda-23.10.0        | 1.3 MB    |                                       |   0% \n",
      "conda-23.10.0        | 1.3 MB    | 4                                     |   1% \u001b[A\n",
      "rfpimp-1.3.2         | 12 KB     | ##################################### | 100% \u001b[A\n",
      "                                                                                \u001b[A\n",
      "                                                                                \u001b[A\n",
      "Preparing transaction: done\n",
      "Verifying transaction: done\n",
      "Executing transaction: done\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install rfpimp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Channels:\n",
      " - defaults\n",
      " - conda-forge\n",
      "Platform: osx-arm64\n",
      "Collecting package metadata (repodata.json): done\n",
      "Solving environment: failed\n",
      "\n",
      "PackagesNotFoundError: The following packages are not available from current channels:\n",
      "\n",
      "  - sklearn.ensemble.forest\n",
      "\n",
      "Current channels:\n",
      "\n",
      "  - defaults\n",
      "  - https://conda.anaconda.org/conda-forge/noarch\n",
      "  - https://conda.anaconda.org/conda-forge/osx-arm64\n",
      "\n",
      "To search for alternate channels that may provide the conda package you're\n",
      "looking for, navigate to\n",
      "\n",
      "    https://anaconda.org\n",
      "\n",
      "and use the search bar at the top of the page.\n",
      "\n",
      "\n",
      "\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "conda install sklearn.ensemble.forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "from rfpimp import *\n",
    "\n",
    "from types import SimpleNamespace\n",
    "def load_boston(return_X_y=False):\n",
    "    \"\"\"Replacement function for loading in Boston House Prices\"\"\"\n",
    "    df = pd.read_csv('boston_house_prices.csv')\n",
    "    X = df.drop(columns=['MEDV'])\n",
    "    y = df['MEDV'].to_numpy()\n",
    "\n",
    "    if return_X_y:\n",
    "        return X, y \n",
    "    \n",
    "    dataset  = SimpleNamespace(data=X, target=y)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def boston():\n",
    "    boston = load_boston()\n",
    "    df = boston.data\n",
    "    y = boston.target\n",
    "    df['y'] = y\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CRIM</th>\n",
       "      <th>ZN</th>\n",
       "      <th>INDUS</th>\n",
       "      <th>CHAS</th>\n",
       "      <th>NOX</th>\n",
       "      <th>RM</th>\n",
       "      <th>AGE</th>\n",
       "      <th>DIS</th>\n",
       "      <th>RAD</th>\n",
       "      <th>TAX</th>\n",
       "      <th>PTRATIO</th>\n",
       "      <th>B</th>\n",
       "      <th>LSTAT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00632</td>\n",
       "      <td>18.0</td>\n",
       "      <td>2.31</td>\n",
       "      <td>0</td>\n",
       "      <td>0.538</td>\n",
       "      <td>6.575</td>\n",
       "      <td>65.2</td>\n",
       "      <td>4.0900</td>\n",
       "      <td>1</td>\n",
       "      <td>296</td>\n",
       "      <td>15.3</td>\n",
       "      <td>396.90</td>\n",
       "      <td>4.98</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.02731</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>6.421</td>\n",
       "      <td>78.9</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>396.90</td>\n",
       "      <td>9.14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.02729</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.07</td>\n",
       "      <td>0</td>\n",
       "      <td>0.469</td>\n",
       "      <td>7.185</td>\n",
       "      <td>61.1</td>\n",
       "      <td>4.9671</td>\n",
       "      <td>2</td>\n",
       "      <td>242</td>\n",
       "      <td>17.8</td>\n",
       "      <td>392.83</td>\n",
       "      <td>4.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.03237</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>6.998</td>\n",
       "      <td>45.8</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>394.63</td>\n",
       "      <td>2.94</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.06905</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.18</td>\n",
       "      <td>0</td>\n",
       "      <td>0.458</td>\n",
       "      <td>7.147</td>\n",
       "      <td>54.2</td>\n",
       "      <td>6.0622</td>\n",
       "      <td>3</td>\n",
       "      <td>222</td>\n",
       "      <td>18.7</td>\n",
       "      <td>396.90</td>\n",
       "      <td>5.33</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      CRIM    ZN  INDUS  CHAS    NOX     RM   AGE     DIS  RAD  TAX  PTRATIO  \\\n",
       "0  0.00632  18.0   2.31     0  0.538  6.575  65.2  4.0900    1  296     15.3   \n",
       "1  0.02731   0.0   7.07     0  0.469  6.421  78.9  4.9671    2  242     17.8   \n",
       "2  0.02729   0.0   7.07     0  0.469  7.185  61.1  4.9671    2  242     17.8   \n",
       "3  0.03237   0.0   2.18     0  0.458  6.998  45.8  6.0622    3  222     18.7   \n",
       "4  0.06905   0.0   2.18     0  0.458  7.147  54.2  6.0622    3  222     18.7   \n",
       "\n",
       "        B  LSTAT  \n",
       "0  396.90   4.98  \n",
       "1  396.90   9.14  \n",
       "2  392.83   4.03  \n",
       "3  394.63   2.94  \n",
       "4  396.90   5.33  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_boston = boston()\n",
    "X, y = df_boston.drop('y', axis=1), df_boston['y']\n",
    "y *= 1000 # y is \"Median value of owner-occupied homes in $1000's\" so multiply by 1000\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Train random forests of different sizes in terms of number of trees\n",
    "\n",
    "In this section we will see that as we increase the number of trees in the ensemble/forest, we should initially see model bias going down, i.e. the predictions getting better. It will asymptotically approach some minimum error on the testing set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to train a random forest  that has a single tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestRegressor(n_estimators=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestRegressor</label><div class=\"sk-toggleable__content\"><pre>RandomForestRegressor(n_estimators=1)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "RandomForestRegressor(n_estimators=1)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=1)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task**: Compute the MAE for the training and the testing set, printing them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train 1287.4, test 3290.2\n"
     ]
    }
   ],
   "source": [
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}, test {mae:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task**: Create a quick loop and run the training and testing cycle above several times to see the variance of the training and testing set errors. You should notice that the test set scores bounce around a lot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train 1004.5, test 2912.7\n",
      "MAE train 1210.9, test 2686.3\n",
      "MAE train 1009.4, test 2946.1\n",
      "MAE train 1577.0, test 2811.8\n",
      "MAE train 990.3, test 2724.5\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "    rf = RandomForestRegressor(n_estimators=1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "    mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "    print(f\"MAE train {mae_train:.1f}, test {mae:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task**: Increase the number of trees (`n_estimators`) to 2, retrain, and print out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train 1214.9, test 2309.3\n",
      "MAE train 1307.1, test 3708.3\n",
      "MAE train 1300.4, test 3214.7\n",
      "MAE train 1108.8, test 2988.7\n",
      "MAE train 1171.8, test 3480.4\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "    rf = RandomForestRegressor(n_estimators=2)\n",
    "    rf.fit(X_train, y_train)\n",
    "    mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "    mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "    print(f\"MAE train {mae_train:.1f}, test {mae:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should notice that the scores don't bounce around as much as they did when you were only training one tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Q.**  Why does the MAE score go down?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### In subsequent iterations, the model may learn from its mistakes in earlier runs, adjusting its predictions to better align with the true underlying patterns in the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task**: Increase the number of trees (`n_estimators`) to 10, retrain, and print out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train 867.6, test 2432.6\n",
      "MAE train 951.9, test 2100.8\n",
      "MAE train 934.1, test 2264.0\n",
      "MAE train 1022.3, test 2081.4\n",
      "MAE train 932.7, test 2299.7\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "    rf = RandomForestRegressor(n_estimators=10)\n",
    "    rf.fit(X_train, y_train)\n",
    "    mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "    mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "    print(f\"MAE train {mae_train:.1f}, test {mae:.1f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Q.**  What do you notice about the MAE scores?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MAE scores are lower with increase in number of trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Q.**  After running several times, what else do you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The MAE scores don't change as much at each iteration as the number of tree increases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task**: Increase the number of trees (`n_estimators`) to 200, retrain, and print out the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train 868.5, test 1863.3\n",
      "MAE train 816.9, test 2742.8\n",
      "MAE train 848.0, test 1877.0\n",
      "MAE train 833.7, test 2318.2\n",
      "MAE train 816.8, test 2231.0\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "    rf = RandomForestRegressor(n_estimators=200)\n",
    "    rf.fit(X_train, y_train)\n",
    "    mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "    mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "    print(f\"MAE train {mae_train:.1f}, test {mae:.1f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Q.**  What do you notice about the MAE scores from a single run?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The scores are much more stable."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "Both training and testing error have dropped, but not as significantly as before, even with 200 trees.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task**: Notice that it took a little bit longer to train.  Do the exact same thing again but this time use `n_jobs=-1` as an argument to the `RandomForestRegressor` constructor.\n",
    "\n",
    "This tells the library to use all processing cores available on the computer processor. As long as the data is not too huge (because it must pass it around), it often goes much faster using this argument. It should take less than two seconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train 792.1, test 2548.3\n",
      "MAE train 833.4, test 2005.3\n",
      "MAE train 835.1, test 1932.3\n",
      "MAE train 829.3, test 2347.1\n",
      "MAE train 782.9, test 2265.2\n"
     ]
    }
   ],
   "source": [
    "for i in range(5):\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n",
    "    rf = RandomForestRegressor(n_jobs=-1, n_estimators=200)\n",
    "    rf.fit(X_train, y_train)\n",
    "    mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "    mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "    print(f\"MAE train {mae_train:.1f}, test {mae:.1f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Q.**  What do you notice about the MAE scores from SEVERAL runs?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variance is even lower."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "The error variance across runs is even lower (tighter).\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Examining model size and complexity\n",
    "\n",
    "The structure of a tree is affected by a number of hyperparameters, not just the data. The goal in this section is to see the effect of altering the number of observations per leaf and the maximum number of candidate features per split. Let's start out with a handy function that uses some  support code from rfpimp to examine tree size and depth:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def showsize(ntrees, max_features=1.0, min_samples_leaf=1):\n",
    "    rf = RandomForestRegressor(n_estimators=ntrees,\n",
    "                               max_features=max_features,\n",
    "                               min_samples_leaf=min_samples_leaf,\n",
    "                               n_jobs=-1)\n",
    "    rf.fit(X_train, y_train)\n",
    "    n = rfnnodes(rf)                # from rfpimp\n",
    "    h = np.median(rfmaxdepths(rf))  # rfmaxdepths from rfpimp\n",
    "    mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "    mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "    print(f\"MAE train {mae_train:6.1f}, test {mae:6.1f} using {n:9,d} tree nodes with {h:2.0f} median tree height\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of number of trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For a single tree, we see about 480 nodes and a tree height of around 19:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train 1033.4, test 2862.7 using       497 tree nodes with 20 median tree height\n"
     ]
    }
   ],
   "source": [
    "showsize(ntrees=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task**: Look at the metrics for 2 trees and then 100 trees."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train 1203.6, test 2914.7 using       942 tree nodes with 18 median tree height\n"
     ]
    }
   ],
   "source": [
    "showsize(ntrees=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train  822.1, test 2367.1 using    47,556 tree nodes with 19 median tree height\n"
     ]
    }
   ],
   "source": [
    "showsize(ntrees=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Q.** Why does the median height of a tree stay the same when we increase the number of trees?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The way the tree is constructed is not impacted by number of trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "While the number of nodes increases with the number of trees, the height of any individual tree will stay the same because we have not fundamentally changed how it is constructing a single tree.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of increasing min samples / leaf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Loop around a call to `showsize()` with 10 trees and min_samples_leaf=1..10 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train  913.5, test 2381.3 using     4,734 tree nodes with 18 median tree height\n",
      "MAE train 1036.4, test 2587.4 using     2,210 tree nodes with 16 median tree height\n",
      "MAE train 1338.1, test 2434.9 using     1,394 tree nodes with 14 median tree height\n",
      "MAE train 1522.2, test 2395.1 using     1,044 tree nodes with 11 median tree height\n",
      "MAE train 1633.1, test 2528.4 using       814 tree nodes with 11 median tree height\n",
      "MAE train 1803.9, test 2685.5 using       666 tree nodes with 11 median tree height\n",
      "MAE train 1829.8, test 2576.8 using       572 tree nodes with 10 median tree height\n",
      "MAE train 1919.9, test 2730.0 using       496 tree nodes with  9 median tree height\n",
      "MAE train 2036.0, test 2472.0 using       426 tree nodes with  9 median tree height\n",
      "MAE train 2085.1, test 2621.5 using       402 tree nodes with  8 median tree height\n"
     ]
    }
   ],
   "source": [
    "for i in range(1,11):\n",
    "    showsize(ntrees=10, min_samples_leaf=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Q.** Why do the median height of a tree and number of total nodes decrease as we increase the number of samples per leaf?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The tree gets splitted less often if the number of samples per leaf is increased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Q.**  Why does the MAE error increase?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The average taken over more observations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "If we include more observations in a single leaf, then the average is taken over more observations. That average is a more general prediction but less accurate.\n",
    "</details> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's pretty clear from that print out that `min_samples_leaf=1` is the best choice because it gives the minimum validation error."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Effect of reducing max_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task:** Do another loop from `max_features` = 4 down to 1, with 1 sample per leaf. (There are 4 total features.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 MAE train  888.4, test 2443.2 using     4,784 tree nodes with 18 median tree height\n",
      "12 MAE train  927.7, test 2255.0 using     4,770 tree nodes with 18 median tree height\n",
      "11 MAE train  865.6, test 2467.5 using     4,880 tree nodes with 18 median tree height\n",
      "10 MAE train  897.1, test 2337.4 using     4,746 tree nodes with 19 median tree height\n",
      " 9 MAE train  909.7, test 2170.1 using     4,812 tree nodes with 18 median tree height\n",
      " 8 MAE train  893.8, test 2565.4 using     4,742 tree nodes with 18 median tree height\n",
      " 7 MAE train  921.1, test 2393.2 using     4,826 tree nodes with 18 median tree height\n",
      " 6 MAE train  960.6, test 2202.9 using     4,768 tree nodes with 19 median tree height\n",
      " 5 MAE train  894.2, test 2342.7 using     4,918 tree nodes with 18 median tree height\n",
      " 4 MAE train  957.9, test 2567.2 using     4,864 tree nodes with 18 median tree height\n",
      " 3 MAE train 1007.1, test 2235.2 using     4,888 tree nodes with 19 median tree height\n",
      " 2 MAE train  992.3, test 2390.1 using     4,910 tree nodes with 19 median tree height\n",
      " 1 MAE train 1294.0, test 2666.4 using     4,932 tree nodes with 19 median tree height\n"
     ]
    }
   ],
   "source": [
    "p = X_train.shape[1]\n",
    "for i in range(p,0,-1):\n",
    "    print(f\"{i:2d} \",end='')\n",
    "    showsize(ntrees=10, max_features=i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this data set, changing the number of candidate features does not change the height of the tree much nor do we see a very clear pattern for the test set error, e.g. the error clearly increasing or decreasing as we decrease the number of candidates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RF prediction confidence\n",
    "\n",
    "A random forest is a collection of decision trees, each of which contributes a prediction. The forest averages those predictions to provide the overall prediction (or takes most common vote for classification). Let's dig inside the random forest to get the individual trees out and ask them what their predictions are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: Train a random forest with 10 trees on `X_train`, `y_train`.  Use `for t in rf.estimators_` to iterate through the trees making predictions with `t` not `rf`. Print out the usual MAE scores for each tree predictor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MAE train 1089.9, test 3200.0\n",
      "MAE train 1064.4, test 3322.5\n",
      "MAE train 1294.8, test 3052.9\n",
      "MAE train 1376.7, test 2987.3\n",
      "MAE train 1123.8, test 3318.6\n",
      "MAE train 1082.7, test 3602.0\n",
      "MAE train 1127.0, test 3590.2\n",
      "MAE train 1406.2, test 3087.3\n",
      "MAE train 1472.3, test 3019.6\n",
      "MAE train 1066.1, test 3005.9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n",
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/base.py:458: UserWarning: X has feature names, but DecisionTreeRegressor was fitted without feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=10, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "for t in rf.estimators_:\n",
    "    mae_train = mean_absolute_error(y_train, t.predict(X_train))\n",
    "    mae = mean_absolute_error(y_test, t.predict(X_test))\n",
    "    print(f\"MAE train {mae_train:.1f}, test {mae:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that it bounces around quite a bit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task**: Select any one of the `X_test` rows and print out the predicted rent price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.43571   0.       10.59      1.        0.489     5.344   100.\n",
      "    3.875     4.      277.       18.6     396.9      23.09   ]] => [16020.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/anaconda3/lib/python3.11/site-packages/sklearn/base.py:465: UserWarning: X does not have valid feature names, but RandomForestRegressor was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "x = X_test.iloc[30,:] # pick single test case\n",
    "x = x.values.reshape(1,-1)\n",
    "print(f\"{x} => {rf.predict(x)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task**: Now let's see how the forest came to that conclusion. Compute the average of the predictions obtained from every tree. Compare that to the prediction obtained directly from the random forest (`rf.predict(X_test)`). They should be the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[  0.43571   0.       10.59      1.        0.489     5.344   100.\n",
      "    3.875     4.      277.       18.6     396.9      23.09   ]] => 16020.0\n"
     ]
    }
   ],
   "source": [
    "y_pred = np.mean([t.predict(x) for t in rf.estimators_])\n",
    "print(f\"{x} => {y_pred}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "y_pred = np.mean([t.predict(x) for t in rf.estimators_])\n",
    "print(f\"{x} => {y_pred}$\")\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task**: Compute the standard deviation of the tree estimates and print that out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4721.39809802139"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.std([t.predict(x) for t in rf.estimators_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<details>\n",
    "<summary>Solution</summary>\n",
    "<pre>\n",
    "np.std([t.predict(x) for t in rf.estimators_])\n",
    "</pre>\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lower the standard deviation, the more tightly grouped the predictions were, which means we should have more confidence in our answer. \n",
    "\n",
    "Different records will often have different standard deviations, which means we could have different levels of confidence in the various answers. This might be helpful to a bank for example that wanted to not only predict whether to give loans, but how confident the model was."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Altering bootstrap size\n",
    "\n",
    "In this section we will tune one final hyperparameter and see how it affects the model: the `max_samples` which controls the size of the bootstrapped data set used for fitting each tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task**: There are only about 400 training records, change that to 200 and check the error again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 368 ms, sys: 7.89 ms, total: 376 ms\n",
      "Wall time: 375 ms\n",
      "MAE train 809.3, test 2277.0\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=200) # don't compute in parallel so we can see timing\n",
    "%time rf.fit(X_train, y_train)\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}, test {mae:.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 261 ms, sys: 5.13 ms, total: 266 ms\n",
      "Wall time: 264 ms\n",
      "MAE train 1376.8, test 2293.6\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=200, max_samples=1/2)\n",
    "%time rf.fit(X_train, y_train)\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}, test {mae:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's a bit less accurate, but it's faster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Q.**  Why is it less accurate?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Less accurate due to smaller sample size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Task**: Turn off bootstrapping by adding `bootstrap=False` to the constructor of the model. This means that it will subsample rather than bootstrap. Remember that bootstrapping gets about two thirds of the data because of replacement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 728 ms, sys: 59.6 ms, total: 787 ms\n",
      "Wall time: 192 ms\n",
      "MAE train 0.0$, test 2975.6$\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestRegressor(n_estimators=200, n_jobs=-1, bootstrap=False)\n",
    "%time rf.fit(X_train, y_train)\n",
    "mae_train = mean_absolute_error(y_train, rf.predict(X_train))\n",
    "mae = mean_absolute_error(y_test, rf.predict(X_test))\n",
    "print(f\"MAE train {mae_train:.1f}$, test {mae:.1f}$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice what happened to the training set error. It got quite a bit lower when we do not do bootstrapping, we are overfitting by a lot."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
